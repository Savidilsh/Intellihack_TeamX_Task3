# -*- coding: utf-8 -*-
"""Question_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BaF204lJvPBnVbs_BpbsW1XYd53vx7Aw
"""

!pip install transformers unsloth torch datasets ragas deepeval
!git clone https://github.com/ggerganov/llama.cpp.git

!pip install pdfplumber

import pdfplumber

# Path to your uploaded PDF
pdf_path = "/content/2501.12948v1.pdf"

# Extract text
deepseek_text1 = ""
with pdfplumber.open(pdf_path) as pdf:
    for page in pdf.pages:
        deepseek_text1 += page.extract_text() or ""  # Handle cases where text extraction fails

print("Loaded text length:", len(deepseek_text1), "characters")
print("Sample text:", deepseek_text1[:500])  # Preview first 500 characters

import os

# List of .md files (adjust names based on what you uploaded)
md_files = [f for f in os.listdir("/content") if f.endswith(".md")]

# Combine content from all .md files
deepseek_text = ""
for md_file in md_files:
    with open(f"/content/{md_file}", "r", encoding="utf-8") as f:
        deepseek_text += f.read() + "\n\n"  # Add newline between files
    print(f"Loaded {md_file}, total length so far: {len(deepseek_text)} characters")

print("Final text length:", len(deepseek_text), "characters")
print("Sample text:", deepseek_text[:500])  # Preview first 500 characters
deepseek_text = deepseek_text+deepseek_text

!pip install langchain transformers torch

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from transformers import pipeline
import torch
import pandas as pd

# Split deepseek_text into chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_text(deepseek_text)
documents = [Document(page_content=chunk) for chunk in chunks]
print(f"Created {len(documents)} document chunks")

# Load distilgpt2 for question generation
generator = pipeline("text-generation", model="distilgpt2", device=0 if torch.cuda.is_available() else -1)

# Generate QA pairs
qa_pairs = []
for doc in documents[:50]:  # Limit to 50 chunks for speed; adjust as needed
    # Generate question
    prompt = f"Generate a question based on this text: {doc.page_content[:500]}"  # Truncate for brevity
    question = generator(
        prompt,
        max_new_tokens=30,  # Generate up to 30 new tokens
        num_return_sequences=1,
        truncation=True
    )[0]["generated_text"].split(":")[-1].strip()
    # Use chunk as answer (simplified)
    answer = doc.page_content[:300]  # First 300 chars as answer
    qa_pairs.append({"question": question, "answer": answer})

# Convert to DataFrame and save
qa_pairs_df = pd.DataFrame(qa_pairs)
qa_pairs_df.to_json("qa_dataset.json")
print("Sample QA pairs:")
print(qa_pairs_df.head())
print("Saved QA dataset to qa_dataset.json")

from datasets import load_dataset

# Load the dataset, specifying the 'dataset' field
dataset = load_dataset('json', data_files='/content/data_set.json', field='dataset')

# Inspect the dataset
print("Dataset structure:", dataset)
print("Number of samples:", len(dataset['train']))
print("First sample:", dataset['train'][0])

# Split into 80% train (40 samples), 20% temp (10 samples)
dataset_split = dataset['train'].train_test_split(test_size=0.2, seed=42)

# Split the 20% temp into 10% val (5 samples), 10% test (5 samples)
test_val_split = dataset_split['test'].train_test_split(test_size=0.5, seed=42)

# Assign the splits
train_dataset = dataset_split['train']
val_dataset = test_val_split['train']
test_dataset = test_val_split['test']

# Verify sizes
print("Train size:", len(train_dataset))  # Should be 40
print("Validation size:", len(val_dataset))  # Should be 5
print("Test size:", len(test_dataset))  # Should be 5

from transformers import AutoTokenizer

model_name = "Qwen/Qwen2.5-3B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def preprocess_function(examples):
    # Format as a simple Q&A prompt
    prompts = [f"### Question: {q}\n### Answer: {a}" for q, a in zip(examples['question'], examples['answer'])]
    return tokenizer(prompts, truncation=True, padding="max_length", max_length=512)

# Tokenize all splits
tokenized_train = train_dataset.map(preprocess_function, batched=True)
tokenized_val = val_dataset.map(preprocess_function, batched=True)
tokenized_test = test_dataset.map(preprocess_function, batched=True)

# Check a tokenized sample
print("Tokenized sample:", tokenized_train[0])

from unsloth import FastLanguageModel
import torch

model_name = "Qwen/Qwen2.5-3B-Instruct"
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model_name,
    max_seq_length=512,
    dtype=torch.float16,  # Explicitly enforce FP16
    load_in_4bit=True,
    device_map="auto"  # Ensure proper device placement
)

# Add LoRA adapters
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_alpha=32,
    lora_dropout=0.05,
    use_gradient_checkpointing=True
)

# Core dependencies for PyTorch and model handling
!pip install torch torchvision

# Hugging Face ecosystem for transformers, datasets, and training
!pip install transformers datasets

# Parameter-efficient fine-tuning (e.g., LoRA via PEFT)
!pip install peft

# Accelerate for distributed training and mixed precision
!pip install accelerate

# BitsAndBytes for 4-bit quantization
!pip install bitsandbytes

# Unsloth for 2x faster fine-tuning (as seen in your logs)
!pip install "unsloth[colab] @ git+https://github.com/unslothai/unsloth.git"

# For GGUF conversion (required for 4-bit quantized model submission)
!pip install git+https://github.com/ggerganov/llama.cpp.git

# Weights & Biases for logging training progress
!pip install wandb

# Optional: Dataset generation tools (RAGAS or DeepEval)
!pip install ragas  # For synthetic dataset generation
# !pip install deepeval  # Uncomment if you prefer DeepEval

# Optional: Evaluation framework (if you implement your own)
!pip install evaluate

from datasets import Dataset
from transformers import AutoTokenizer

# Load tokenizer
model_name = "Qwen/Qwen2.5-3B-Instruct"  # or "Qwen/Qwen2.5-3B"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Example synthetic dataset (replace with your actual data)
data = {"text": ["Sample text for fine-tuning", "Another example sentence"]}
dataset = Dataset.from_dict(data)

# Tokenize and prepare inputs with labels
def preprocess_function(examples):
    tokenized = tokenizer(examples["text"], truncation=True, padding="max_length", max_length=128)
    tokenized["labels"] = tokenized["input_ids"].copy()  # Labels are the same as input_ids
    return tokenized

train_dataset = dataset.map(preprocess_function, batched=True)

from transformers import AutoModelForCausalLM, TrainingArguments, Trainer
from unsloth import FastLanguageModel
import torch

# Load model with 4-bit quantization
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model_name,
    max_seq_length=2048,  # Explicitly set to avoid previous max_seq_length error
    dtype=torch.float16,
    load_in_4bit=True,
)

# Add LoRA adapters for efficient fine-tuning
model = FastLanguageModel.get_peft_model(
    model,
    r=16,  # LoRA rank
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_alpha=32,
    lora_dropout=0,
)

# Training arguments
training_args = TrainingArguments(
    output_dir="./qwen_finetuned",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    logging_dir="./logs",
    report_to="wandb",
    fp16=True,  # Mixed precision for efficiency
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    processing_class=tokenizer,  # Modern argument instead of deprecated `tokenizer`
)

# Train
trainer.train()

# Test model output
sample = train_dataset[0]
device = next(model.parameters()).device  # Get the device the model is on (e.g., cuda:0)
inputs = {
    "input_ids": torch.tensor([sample["input_ids"]], device=device),
    "attention_mask": torch.tensor([sample["attention_mask"]], device=device),
    "labels": torch.tensor([sample["labels"]], device=device),
}
outputs = model(**inputs)
print(outputs.keys())  # Should include 'loss'

from transformers import TrainingArguments, Trainer
from unsloth import FastLanguageModel
import torch
from datasets import Dataset

# Load model and tokenizer
model_name = "Qwen/Qwen2.5-3B-Instruct"  # or "Qwen/Qwen2.5-3B"
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model_name,
    max_seq_length=2048,
    dtype=torch.float16,
    load_in_4bit=True,
)

# Add LoRA adapters
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_alpha=32,
    lora_dropout=0,
)

# Example synthetic dataset (replace with your actual data)
data = {"text": ["Sample text for fine-tuning", "Another example sentence"]}
dataset = Dataset.from_dict(data)

# Preprocess dataset
def preprocess_function(examples):
    tokenized = tokenizer(examples["text"], truncation=True, padding="max_length", max_length=128)
    tokenized["labels"] = tokenized["input_ids"].copy()  # Labels for causal LM
    return tokenized

train_dataset = dataset.map(preprocess_function, batched=True)

# Training arguments
training_args = TrainingArguments(
    output_dir="./qwen_finetuned",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    logging_dir="./logs",
    report_to="wandb",
    fp16=True,  # Mixed precision
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    processing_class=tokenizer,
)

# Train
trainer.train()

# Save quantized model
model.save_pretrained_gguf("./qwen_finetuned_gguf", tokenizer, quantization_method="q4_k_m")

print(train_dataset[0]["input_ids"])  # Should be a list or numpy array, not a tensor yet

from transformers import TrainingArguments, Trainer
from unsloth import FastLanguageModel
import torch

# Load model and tokenizer
model_name = "Qwen/Qwen2.5-3B-Instruct"
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model_name,
    max_seq_length=2048,
    dtype=torch.float16,
    load_in_4bit=True,
)

# Add LoRA adapters
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_alpha=32,
    lora_dropout=0,
)

# Training arguments
training_args = TrainingArguments(
    output_dir="./qwen_finetuned",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    logging_dir="./logs",
    report_to="wandb",
    fp16=True,
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    processing_class=tokenizer,
)

# Train
trainer.train()

# Save quantized model
model.save_pretrained_gguf("./qwen_finetuned_gguf", tokenizer, quantization_method="q4_k_m")

from transformers import TrainingArguments, Trainer
from unsloth import FastLanguageModel
import torch
from datasets import Dataset

# Load model and tokenizer
model_name = "Qwen/Qwen2.5-3B-Instruct"  # or "Qwen/Qwen2.5-3B"
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model_name,
    max_seq_length=2048,
    dtype=torch.float16,
    load_in_4bit=True,
)

# Add LoRA adapters
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_alpha=32,
    lora_dropout=0,
)

# Example synthetic dataset (replace with your actual data)
data = {"text": ["Sample text for fine-tuning", "Another example sentence"]}
dataset = Dataset.from_dict(data)

# Preprocess dataset
def preprocess_function(examples):
    tokenized = tokenizer(examples["text"], truncation=True, padding="max_length", max_length=128)
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

train_dataset = dataset.map(preprocess_function, batched=True)
print(train_dataset[0])  # Verify structure

# Training arguments
training_args = TrainingArguments(
    output_dir="./qwen_finetuned",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    logging_dir="./logs",
    report_to="wandb",
    fp16=True,
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    processing_class=tokenizer,
)

# Train
trainer.train()

# Save quantized model
model.save_pretrained_gguf("./qwen_finetuned_gguf", tokenizer, quantization_method="q4_k_m")

"""***FROM HERE ONWARDS MODEL WILL BE TRAINED USING A DATA SET MADE BY HAND WITH 50 Q&As.***

"""

import torch
from datasets import Dataset
from transformers import TrainingArguments, Trainer
from unsloth import FastLanguageModel
import os
import re

# 1. Clean Text to Handle Surrogates and Unicode Escapes
def clean_text(text):
    # Replace common Unicode escapes with their actual characters or remove them
    text = re.sub(r'\\ud[0-9a-f]{3}', '', text)  # Remove unpaired surrogates like \ud83d
    text = re.sub(r'\\u([0-9a-fA-F]{4})', lambda m: chr(int(m.group(1), 16)), text)  # Decode valid \uXXXX
    # Ensure only valid UTF-8 characters remain
    return text.encode('utf-8', errors='ignore').decode('utf-8')

# 2. Define the JSON Data In-Memory and Clean It
def get_dataset():
    data = {
        "question": {
            "0": "we have a big business on all fronts. And that is why we're making this year's big reveal.",
            "1": "High-Fetch and deepEP support in H800\n\ud83d\udd17 DeepEP GitHub Repo",
            "2": "## Day 3 - DeepGEMM\n\nIntroducing DeepGEMM - an FP8 GEMM library that supports both dense and MoE GEMMs, powering V3\/R1 training and inference.\n\n\ud83d\udd17 DeepGEMM GitHub Repo\n\u26a1 Up to 1350+ FP8 TFLOPS on Hopper GPUs\n\u2705 No heavy dependency, as clean as a tutorial\n\u2705 Fully Just-In-Time compiled\n\u2705 Core logic at ~300 lines - yet outperforms expert-tuned kernels across most matrix sizes\n\u2705 Supports dense layout and two MoE layouts\n\n## Day 4 - Optimized Parallelism Strategies\n\n\u2705 DualPipe - a bidirectional pipelinem based system and an extensible library, as a standalone package for OpenASM GPUs\n## Day 5 - Optimized FPGA, fast",
            "3": "## Day 5 - 3FS, Thruster for All DeepSeek Data Access\n\nFire-Flyer File System (3FS) - a parallel file system that utilizes the full bandwidth of modern SSDs and RDMA networks.\n\n\u26a1 6.6 TiB\/s aggregate read throughput in a 180-node cluster\n\u26a1 3.66 TiB\/min throughput on GraySort benchmark in a 25-node cluster\n\u26a1 40+ GiB\/s peak throughput per client node for KVCache lookup\n\ud83e\uddec Disaggregated architecture with strong consistency semantics\n\u2705 Training data preprocessing, dataset loading, checkpoint saving\/reloading, etc\n\u2705 Preprocessing data preprocessing, dataset loading, checkpoint saving\/reloading, etc\n\u260d Batch size is based",
            "4": "\/\/medium.com\/@jjjy213\/deepseek-v3-explained-fdac83ba280c\"\/>\nauthor - Ataka jeong.wang@gmail.com",
            "5": "1. Introduction\nHow could the DeepSeek-V3 model achieve incredible performance and economical training as an open source model? In this paper review, we will explore the various features that were invented and applied to build the DeepSeek-V3 model. The way the paper presents the model may seem complicated to people who are unfamiliar with the new conceptualization invented by DeepSeek. However, its core principle still resembles that of the standard Transformer and well-known LLMs. It will be i.e. the two basic concept of the DeepSeek-V3. The first part will concentrate on the concept of the neural network. The",
            "6": "Let\u2019s dive into the new features of model architecture step by step.\n2. Model Architecture\nFirst of all, we will investigate the core architecture of DeekSeek-V3 model. The DeekSeek-V3 model has inherited most parts of model from previous V2 model. These parts of model were elaborated more in V2 paper, but it is worth noting the principle how V3 model was built upon. While they used the structure of the ordinary transformer block like Llama, its attention and Feed-Forward Network were more sophiagnosed on the basis of DeekSeek-V2, they still thought it was necessary to build an implementation that was much more complex than",
            "7": "a continuous source dataset, and a series of random results. The first is",
            "8": "DeepSeek model can improve both speed by reducing data copying and memory efficiency by using a smaller compressed vector. The weight matrix for compression is additionally required, because human cannot compress it manually, but AI should learn it how the compression should be done. Applying RoPE to the compressed vector is not mathematically compatible, which was shown in detail in V2 paper, they used decoupled RoPE. As illustrated in the figure above, RoPE is applied to query and key, but als-Rasim and Saqr will write a subset of the output value depending on how much RoPE the algorithm can take (e.g",
            "9": "- 2.2 DeekSeekMoEcEwHcDpZKcZg+VQRzYmQcLK+hcN",
            "10": "Secondly, you can note that Feed-Forward Network is unusual as it was split into a lot of experts, rather than one large FFN. They called it as DeekSeekMoE. Like humans in a group, the AI also needs to specialized in certain domain to improve the performance. Thus, the mixture of experts come into play here. Each expert can specialize in certain domain, in this case, the group of tokens that they are familiar with, instead of coping with entire range of tokens alone. Dependent on the input sequexers (meaning all the tokens that have been allocated to each other, regardless of the size of the token), they can also specialize in certain domain",
            "11": "\/\/\n\/\/\n\/\/\n\/\/\n\/\/\n\/\/\n\/\/\n\/\/\n\/\n\/\/",
            "12": "e\u1d62 is a centroid vector. It is learned during training and represents the type of input tokens the expert specialized in. Each expert\u2019s centroid vector encodes the knowledge domain it specializes in.\nu\u209c is input vector to FFN. The dot product u\u209c\u1d40 e\u1d62 quantifies the similarity between the input vector u\u209c\u200b and the centroid (or domain) of expert e\u1d62, effectively measuring the alignment of the input data with the expert\u2019s specialized domain. So, the s\u1d62 = Sigmoid(u\u209c\u1d40 e\u1d62) represents the score for each i\u209c\u1d40 e\u1d62 as the d value points from the expert\u2019s centroid vector. In other words, the s\ufffd",
            "13": "2.3 Multi-Token Prediction\nIn a standard transformer, the model generates a token each time, and this new token is fed back into decoder as input. Since this way restricts the efficiency and the speed of convergence during training, many researchers have made effort to come up with a method to generate multiple tokens each time. DeepSeek improved the conventional way of Multi-Token Prediction(MTP). Instead of previous parallel MTP, DeepSeek decided sequential MTP. They construct independent MTP ids, as input and outputs, such as the output and output. By using the Multi-token Prediction, the results of which are published in the",
            "14": "As shown in the figure, the structure of MTP modules is akin to RNN model. But, unlike RNN, which preserve hidden states of nodes, the MTP modules send output of prediction to the subsequent module. Even though a single Transformer block cannot generate multiple tokens, the entire system of MTP modules collectively enables multi-token prediction. As it compares additional tokens per prediction, it provides more information for weight updates during training, leading to more efficient learning an overall view on how to train correctly. This was illustrated by this example of a MSP for training with two Transformer blocks.\n\n\nFigure",
            "15": "3. Infrastructure\n3.1 DualPipe\nSince the U.S. did not export great GPUs like the NVIDIA H100 to China, DeepSeek researchers had to devise innovative methods to accelerate model training using the weaker H800 GPUs. Since they succeeded, NVIDIA\u2019s stock price briefly plunged, as people believed that high-performance GPUs would no longer be necessary for training LLM. Because the DeepSeek model was trained on 2048 H800 GPUs, communication between GPUs accounts for large portion of training time. The same process can lead to a lot of errors based on the H800, not only on memory speed and throughput, but also on the size of the",
            "16": "During model training, data flows through the model in forward and backward processes. In forward process, data goes from the input layer to the output layer. On the other hand, during the backward process data moves from the output layer to the input later, updating weights based on the information to minimize the loss. Prior to DeepSeek, researchers found that the backward process can be split into two processes, which are backward for input and backward for weight, in order to remove more bubbling about to the bottom layer from the model. In fact, many research subjects still use the backward process if they have to use a forward process to",
            "17": "In such process, it is certain that an enormous number of communications between GPUs is required. In order to reduce the number of communication, the DeepSeek\u2019s DualPipe combines the forward process and the backward for input by initiating training data from two devices in the opposite directions as illustrated in following figure.\n\n\n\n\u201c For example the GPU to be able to send messages between the GPU and the GPU, the GPU must be able to send",
            "18": "The batch 0 is the initial data, which starts processing on the device 0 and continues on the subsequent devices. In a conventional training plan, the device 7 remains idle, waiting for the batch 0 to be copied onto it. However, DualPipe makes the device 7 start training with other batch data in the opposite direction. This allows us to combine them as a chunk and continuously copy them together on other devices to reduce communication between GPUs. With weaker H800 GPUs, they couldn\u2019t improve t\/m for an even longer time. The final result of this design would not depend on dualPipe hardware, however it would be easier for us",
            "19": "3.2 Mixed precision training\nMixed precision training is already prevalent technique of LLM to improve training and memory efficiency while maintaining the model accuracy. In mixed precision training, it is critical task to find out which parts of model are less significant for the model accuracy and reduce the precision that parts. In DeepSeek-V3 model, the researchers have found that they should reduce precision in the parts of model where heavy computations are executed, such as matrix multiprocessors or in the case of the neural networks that run parallel processing. As demonstrated in the research, the techniques that reduce precision in machine learning",
            "20": "While reducing the precision by the method above, overflow and underflow arise as a impediment. If the numerical values are quantized in lower precision like FP8 format, the values are clipped to the certain representable range. While computation in lower precision, the values can easily exceed the range during the computation. Scaling the values can mitigate the overflow and underflow by adjusting the values and lead to more proper representation in limited range. But, static scaling, which appends an iterable array using a single call, is often a more common problem in computation. In this case, the two types of scaling are defined",
            "21": "Another issue of quantization is that the small errors can be accumulated and become more serious problem later. In order to avoid that a lot of values with error are summed and their errors are accumulated, intermediate values are copied in high precision, if the number of values reaches the interval. It means that some values are grouped, and their values are stored in high precision. Then, the errors of values aren\u2019t accumulated on a large scale, because the small group of values don\u2019t contrivaate. This is where the problem arises.\nWhat happens if a group of values gets over a small number of values?\nAs soon as",
            "22": "4. Reinforcement Learning\nAfter supervised fine-tuning, DeepSeek additionally implemented reinforcement learning. A reward model has to be built and trained for reinforcement learning, which gives feedback to the model and determine the direction of learning. The rule-based reward model(RM) and model-based reward model(RM) were employed.\nThe rule-based RM is applied to the questions with specific rules, such as math problems and LeetCode problems. In these domains, the specific rules are used to reward each participant.\nThe results for all training were presented to Meeves to make sure that the rewards were sufficiently random to be the prediction value",
            "23": "DeepSeek-V3 model, as V2 model did, adopted Group Relative POlicy Optimization (GRPO). This GRPO algorithm maximizes the following objective by updating the policy model \u03c0.\n\n\n\nAs such, we will examine the general structure, as well as its impact on a variety of problems. We will focus on this",
            "24": "Maximize this objective by updating the weights of the model based on the reward.\n\n\n\n\nThe most important part of the experiment may therefore be to find the correct model and achieve a value within the model. There is",
            "25": "Advantage is defined as the normalized reward. The reward is always calculated by the value of the probability that a simple test finds that a simple test is successful by adding a more complex test. This",
            "26": "In LLM case, the policy model \u03c0 is model itself, and \u03b8 is weights of the model. q is question and o is output of the model. We can interpret the policy model(LLM) outputs a probability distribution over tokens, where the policy \u03c0(o|q) is a probability of output o given the question q. Therefore, the policy model is LLM itself. If the output o is right answer, we should reinforce the probability of that model makes this output o. So we need to maximize \u03c0(o|q) by multiplying advantage(normalized r) with advantage(or (normalized zb) of the policy model. Here the policy model outputs \u03c0(o|q) and y",
            "27": "might cause model to forget basic language understanding and important knowledge that the model learned during pre-training and fine-tuning. To implement this safety concerns, GRPO algorithm used KL divergence and epsilon parameter. The KL divergence measures the difference between current policy model and reference policy model(initial base model). So the KL divergence term should be minimized to maximized the GRPO objective. And we pick minimum between the original policy and the clipped policin.\nSince this is an open source application, you can follow all my previous posts on Github.\nThis is a pretty simple module to get",
            "28": "Deep",
            "29": "A Game-Changer in A.I. Here\u2019s Why It Matters\n\nCurrently, the AI models from the Chinese startup Deepseek are causing quite a stir in the AI space. Their latest reasoning model, Deepseek r1, shows better or equal performance to competitors. But above all, they achieved it with a fraction of the training and inference cost.\nDeepSight is the only known proof of this fact, and is the most fundamental proof of how AI works.\nThe problem for us is that the AI",
            "30": "## So what made Deepseek such a big impact to A.I. ?\nThe significance of Deepseek as a disruptor in the industry lies in its approach. Unlike other companies that pushed for better hardware, Deepseek improved the algorithms. Thus achieving better results at a software level.\nNote that the following details are for the Deepseek V3 model.\n\u2022 Deepseek said it trained a model using a data centre of some 2,000 of Nvidia H800 GPUs.\n\u2022 Time duration 2 months with the cost of the *final training run being 6\u00bd months.\n\u2022 In this model, it trained an algorithm to find it only after the \u202binitial validation period\u2021 (approximately 17",
            "31": "### Deepseek made training more efficient (45 times more efficient)\n- Use 8-bit instead of 32-bit to save memory.\n- Compress key value indices which eat up a lot of VRAM; they got 93% compression ratios.\n- Do multi-token prediction instead of single-token prediction -> doubled inference speeds\n- The MOE model decomposes a big model into small models that can run on consumer-grade hardware. We also have multiple tests to try and find better ways to minimize performance.\nIn all cases, many of the tasks for our demo were to run",
            "32": "The first model uses the LDA model, where only one key property is the number of pixels an object has to be encoded in which the result must",
            "33": "DualPipe algorithm for efficient pipeline parallelism. This reduces pipeline bubblers and improves efficiency through comparison over parallelism.\nThey made use of a high-level parallel optimizer such as an async model and a single",
            "34": "using MoE architecture, implementing FP8 mixed precision training, and developing a custom HAI-LLM framework.\n- Deepseek excels at reasoning and math, surpassing GPT-4 and Claude 3.5 Sonnet.\n- For writing and coding tasks, Claude 3.5 Sonnet maintains a sligh-less architecture that can be scaled down to a 4.4-3.5-5.5-5.5.5.5-",
            "35": "\/\/www.h100.org\/h100)\n(0) 1\/12\n(1) 20\/20\/20",
            "36": "- Per Trillion-Dollar \u2020\n- The average maximum level\n- 10.4M GPU hours per training trainer of 3 minutes, 8.44-",
            "37": "$3+13.99M GPU",
            "38": "3K GPU hours\nFinal Fantasy XV is not the first title to receive a new title. For the majority of",
            "39": "cluster manager, metadata service, storage service and client. All components are connected in an RDMA network (InfiniBand or RoCE).\n\nMetadata and storage services send heartbeats to cluster manager. Cluster manager handles membership changes and distributes cluster configuration to other services and clients. Multiple cluster managers are deployed and one of them is elected as the primary. Another manager is promo.\nIf you have any questions, feel free to send a post to the mailing list.\nTo request a discussion on DMR, please contact",
            "40": "File metadata operations (e.g. open or create files\/directories) are sent to metadata services, which implement the file system semantics. Metadata services are stateless, since file metadata are stored in a transactional key-value store (e.g. FoundationDB). Clients can connect to any metadata service.\n\nEach storage service manages a few local SSDs and provides a chunk store interface. The storage service implements Chain Replication with Apportioned Queries (CRAQ) to ensure strong consistency. ~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~\nWith CRAQ the data is stored in the DataStore. There are two way- to store the data and",
            "41": "## File system interfaces\n\nObject store is becoming a popular option for data analytics and machine learning. However, file system semantics and a unified namespace where files are organized in directories provide greater flexibility for applications.\n\n-   *Atomic directory manipulation* An object store can approximate hierarchical directory structures by using slashes (\/) in object keys. However, it doesn\u2019t natively support operations like atomically moving files\/directories, or recursively delisting a new file into directory structure. This method is most common in the filesystems but is more commonly used to identify files with special features.",
            "42": "-   *Symbolic and hard links* Our applications utilize symbolic and hard links to create lightweight snapshots of dynamically updated datasets, where new data is appended as individual files.\n\n-   *Familiar interface* The file interface is well known and used everywhere. There is no need to learn a new storage API. Many datasets are stored as CSV\/Parquet files. Adapting file-based data loaders to use the 3FS FUSE client or native client is straightforward.\n\n### Limitations of FUSE\n\nFUSE (Filesys) is a powerful and fast distributed cloud computing library that focuses on speed. The main goal of FUSE is to deliver the best possible data management tools",
            "43": "-   *Memory copy overhead* The user-space file system daemon cannot access application memory. Data transfer between kernel and user spaces consumes memory bandwidth and increases end-to-end latency.\n\n-   *Primitive multi-threading support* When an application initiates I\/O requests, FUSE places these requests into a multi-threaded shared queue, protected by a spin lock. The user-space file system daemon then retrieves and processes requests from this queue. Due to lock contention, FUSE\u2019s I\/O primes the application's memory.\n-  *Process-safe scheduling* The I\/O threads are periodically started in this process to allow an exception",
            "44": "Most applications, e.g. data analytics, perform large block writes on 3FS or they can buffer data in memory and flush it to 3FS when write buffer is full. However, FUSE on Linux 5.x does not support concurrent writes to the same file[^1]. Applications overcome this limitation by writing to multiple files concurrently, maximizing the total throughput.\n\nRead operations exhibit more complex patterns. Some training jobs require random access to dataset samples, with read sizes varying from a few kilobytes to several kilobytes. Such data analyses can be useful in large-scale databases. With large open databases and many databases, the most",
            "45": "### Asynchronous zero-copy API\n\nImplementing the file system client as a VFS kernel module avoids performance issues mentioned above. But kernel module development is significantly more challenging than user-space system programming. Bugs are difficult to diagnose and can lead to catastrophic failures in production environments. For example, machines may crash and leave no log message for debugging. When upgrading a kernel module, all processes using the file system must be stopped cleanly; otheoretical errors are only caused when the machine has been restarted using the \"unblockchain\" feature. The new version of the kernel implements only",
            "46": "For these reasons, we have chosen to implement a native client within the FUSE daemon. This client offers an interface that supports asynchronous zero-copy I\/O operations. File meta operations are still handled by FUSE daemon (e.g. open\/close\/stat files). Applications call `open()` to obtain a file descriptor (fd) and register it via native API. They can then perform I\/O operations on the file with native client. This approach ensures consistency in metadata operations with the POSIX API, making it possible for existing applications to access native client applications at the same time.\n\n\n\nThe syntax for this work is slightly simplified compared to other",
            "47": "-   *Ior* A small shared ring buffer for communication between user process and native client. The usage of Ior is similar to Linux `io_uring`, where the user process enqueues read\/write requests, and the native client dequeues these requests for completion. The requests are executed in batches, with their sizes controlled by the `io_depth` parameter. Multiple batches are processed in parallel, whether from different rings or the same ring. However, multiple rings are still recommended for multi-purpose use, one that provides access to and allows the user to send messages. The user can specify whether their connection is not required by default,",
            "48": "-b \"|A\"",
            "49": "When an application opens a file, the client contacts the meta service to obtain the file\u2019s data layout information. Then the client can independently compute chunk IDs and chains for data operations, minimizing the involvement of the meta service in the critical path.\n\n### File metadata on transactional key-value store\n\n3FS uses FoundationDB as its distributed storage system for metadata. FoundationDB provides a key-value store interface and supports transactions with Serializable Snapshot Isolations.\n5FS creates a new storage API by implementing a new feature that combines serializing and decoupling of metadata from the underlying storage APIs"
        },
        "answer": {
            "0": "# 202502 Open-Source Week\n\nWe're a tiny team @deepseek-ai pushing our limits in AGI exploration.\n\nStarting this week , Feb 24, 2025 we'll open-source 5 repos \u2013 one daily drop \u2013 not because we've made grand claims, but simply as developers sharing our small-but-sincere progress with full transparency",
            "1": "Stay tuned \u2013 let's geek out in the open together.\n\n## Day 1 - FlashMLA\n\nEfficient MLA Decoding Kernel for Hopper GPUs\nOptimized for variable-length sequences, battle-tested in production\n\n\ud83d\udd17 FlashMLA GitHub Repo\n\u2705 BF16 support\n\u2705 Paged KV cache (block size 64)\n\u26a1 Performance: 3000 GB\/s memory-bound | B",
            "2": "## Day 3 - DeepGEMM\n\nIntroducing DeepGEMM - an FP8 GEMM library that supports both dense and MoE GEMMs, powering V3\/R1 training and inference.\n\n\ud83d\udd17 DeepGEMM GitHub Repo\n\u26a1 Up to 1350+ FP8 TFLOPS on Hopper GPUs\n\u2705 No heavy dependency, as clean as a tutorial\n\u2705 Fully Just-In-Time compiled\n\u2705 Core logic at ~",
            "3": "## Day 5 - 3FS, Thruster for All DeepSeek Data Access\n\nFire-Flyer File System (3FS) - a parallel file system that utilizes the full bandwidth of modern SSDs and RDMA networks.\n\n\u26a1 6.6 TiB\/s aggregate read throughput in a 180-node cluster\n\u26a1 3.66 TiB\/min throughput on GraySort benchmark in a 25-node cl",
            "4": "## Day 6 - One More Thing: DeepSeek-V3\/R1 Inference System Overview\n\nOptimized throughput and latency via:\n\ud83d\udd27 Cross-node EP-powered batch scaling\n\ud83d\udd04 Computation-communication overlap\n\u2696\ufe0f Load balancing\n\nProduction data of V3\/R1 online services:\n\u26a1 73.7k\/14.8k input\/output tokens per second per H800 node",
            "5": "1. Introduction\nHow could the DeepSeek-V3 model achieve incredible performance and economical training as an open source model? In this paper review, we will explore the various features that were invented and applied to build the DeepSeek-V3 model. The way the paper presents the model may seem comp",
            "6": "Let\u2019s dive into the new features of model architecture step by step.\n2. Model Architecture\nFirst of all, we will investigate the core architecture of DeekSeek-V3 model. The DeekSeek-V3 model has inherited most parts of model from previous V2 model. These parts of model were elaborated more in V2 pap",
            "7": "What is Multi-Head Latent Attention(MLA)? You might noticed that \u201cLatent\u201d is only additional word to conventional attention module. MLA improved the speed and memory usage in the attention block by compressing the input vector. From a data analysis perspective, the data can be compressed into a lowe",
            "8": "DeepSeek model can improve both speed by reducing data copying and memory efficiency by using a smaller compressed vector. The weight matrix for compression is additionally required, because human cannot compress it manually, but AI should learn it how the compression should be done. Applying RoPE t",
            "9": "- 2.2 DeekSeekMoE",
            "10": "Secondly, you can note that Feed-Forward Network is unusual as it was split into a lot of experts, rather than one large FFN. They called it as DeekSeekMoE. Like humans in a group, the AI also needs to specialized in certain domain to improve the performance. Thus, the mixture of experts come into p",
            "11": "determines the range of tokens(domain) that experts can deal with well. And we give score to each expert to check how similar the domain of expert and input token are. If the score is high, then we should select the expert and let them activated to make output. Well, it sounds quite simple. Let\u2019s se",
            "12": "e\u1d62 is a centroid vector. It is learned during training and represents the type of input tokens the expert specialized in. Each expert\u2019s centroid vector encodes the knowledge domain it specializes in.\nu\u209c is input vector to FFN. The dot product u\u209c\u1d40 e\u1d62 quantifies the similarity between the input vector",
            "13": "2.3 Multi-Token Prediction\nIn a standard transformer, the model generates a token each time, and this new token is fed back into decoder as input. Since this way restricts the efficiency and the speed of convergence during training, many researchers have made effort to come up with a method to gener",
            "14": "As shown in the figure, the structure of MTP modules is akin to RNN model. But, unlike RNN, which preserve hidden states of nodes, the MTP modules send output of prediction to the subsequent module. Even though a single Transformer block cannot generate multiple tokens, the entire system of MTP modu",
            "15": "3. Infrastructure\n3.1 DualPipe\nSince the U.S. did not export great GPUs like the NVIDIA H100 to China, DeepSeek researchers had to devise innovative methods to accelerate model training using the weaker H800 GPUs. Since they succeeded, NVIDIA\u2019s stock price briefly plunged, as people believed that hi",
            "16": "During model training, data flows through the model in forward and backward processes. In forward process, data goes from the input layer to the output layer. On the other hand, during the backward process data moves from the output layer to the input later, updating weights based on the information",
            "17": "In such process, it is certain that an enormous number of communications between GPUs is required. In order to reduce the number of communication, the DeepSeek\u2019s DualPipe combines the forward process and the backward for input by initiating training data from two devices in the opposite directions a",
            "18": "The batch 0 is the initial data, which starts processing on the device 0 and continues on the subsequent devices. In a conventional training plan, the device 7 remains idle, waiting for the batch 0 to be copied onto it. However, DualPipe makes the device 7 start training with other batch data in the",
            "19": "3.2 Mixed precision training\nMixed precision training is already prevalent technique of LLM to improve training and memory efficiency while maintaining the model accuracy. In mixed precision training, it is critical task to find out which parts of model are less significant for the model accuracy an",
            "20": "While reducing the precision by the method above, overflow and underflow arise as a impediment. If the numerical values are quantized in lower precision like FP8 format, the values are clipped to the certain representable range. While computation in lower precision, the values can easily exceed the ",
            "21": "Another issue of quantization is that the small errors can be accumulated and become more serious problem later. In order to avoid that a lot of values with error are summed and their errors are accumulated, intermediate values are copied in high precision, if the number of values reaches the interv",
            "22": "4. Reinforcement Learning\nAfter supervised fine-tuning, DeepSeek additionally implemented reinforcement learning. A reward model has to be built and trained for reinforcement learning, which gives feedback to the model and determine the direction of learning. The rule-based reward model(RM) and mode",
            "23": "DeepSeek-V3 model, as V2 model did, adopted Group Relative POlicy Optimization (GRPO). This GRPO algorithm maximizes the following objective by updating the policy model \u03c0.",
            "24": "Maximize this objective by updating the weights of the model based on the reward.",
            "25": "Advantage is defined as the normalized reward.",
            "26": "In LLM case, the policy model \u03c0 is model itself, and \u03b8 is weights of the model. q is question and o is output of the model. We can interpret the policy model(LLM) outputs a probability distribution over tokens, where the policy \u03c0(o|q) is a probability of output o given the question q. Therefore, the",
            "27": "might cause model to forget basic language understanding and important knowledge that the model learned during pre-training and fine-tuning. To implement this safety concerns, GRPO algorithm used KL divergence and epsilon parameter. The KL divergence measures the difference between current policy mo",
            "28": "5. Conclusion\nDeepSeek-V3 model offered great opportunity for efficient training with cheaper GPUs. It is unclear that its performance exceeds the OpenAI model, but DeepSeek is way more economical to train and open-source model. AI researchers can directly use DeekSeek models and they can also imple",
            "29": "<source name=\"https:\/\/medium.com\/@visithkumarapperuma\/deepseek-v3-a-game-changer-in-a-i-heres-why-it-matters-75591957ca07\">\n\nauthor - Visith Kumarapperuma\n\n# Deepseek V3: A Game-Changer in A.I. Here\u2019s Why It Matters\n\nCurrently, the AI models from the Chinese startup Deepseek are causing quite a stir",
            "30": "## So what made Deepseek such a big impact to A.I. ?\nThe significance of Deepseek as a disruptor in the industry lies in its approach. Unlike other companies that pushed for better hardware, Deepseek improved the algorithms. Thus achieving better results at a software level.\nNote that the following ",
            "31": "### Deepseek made training more efficient (45 times more efficient)\n- Use 8-bit instead of 32-bit to save memory.\n- Compress key value indices which eat up a lot of VRAM; they got 93% compression ratios.\n- Do multi-token prediction instead of single-token prediction -> doubled inference speeds\n- The",
            "32": "## Summary of how Deepseek v3 was so efficient at training the frontier model\n1. Model Architecture\nThe model employs a Mixture-of-Experts (MoE) architecture, where only 37B parameters fire for each token out of the total 671B. This sparse activation significantly reduces compute requirements compar",
            "33": "They use fine-grained quantisation strategies and increased accumulation precision to maintain accuracy.\n3. Load Balancing Strategy\nThey pioneered an auxiliary loss-free strategy for load balancing in the MoE architecture. This improved performance without the drawbacks of traditional auxiliary loss",
            "34": "## Breakdown of the costs of the Deepseek v3 model\nDeepseek\u2019s flagship model v3 showcases an architecture with a 671B parameter MOE (Mixture of Agents) with 37B active parameters per token\n- Their success stems from breakthrough engineering: using MoE architecture, implementing FP8 mixed precision t",
            "35": "1. Underlying FLOP calculations\nModel Details:\n- Active Parameters: 37B (using FP8 precision)\n- FLOPs per token: Using the rule of thumb \u201c6 FLOPs per parameter per token.\u201d\n`37B\u00d76 = 222B FLOPs per token`\n- Total Training Tokens: Approximately 14.8 trillion tokens\n- Total FLOPs required:\n`222 B FLOPs/",
            "36": "Reference Model: Llama 3.1 (405B parameters, 15 T tokens) reportedly required 30.84 M GPU hours in practice.\nRecalculating FLOPs for Llama 3.1:\n`Using the same math: 3.64\u00d710\u00b2\u2075 FLOPs required`\nScaling Efficiency\nUsing the ratio of FLOPs needed for DeepSeek\u2011V3 versus Llama 3.1. and assuming similar in",
            "37": "`2,664 K+119 K+5 K\u22482.788M GPU hours`\n4. Cost Estimation\nAssumed GPU Rental Price: $2 per GPU hour\nTotal Rental Cost:\n`2.788M GPU hours\u00d7$2\/hour\u2248$5.576 million`\nas stated in Deepseek paper\nDuring the pre\u2011training stage, training DeepSeek\u2011V3 on each trillion tokens requires only 180K H800 GPU hours\u2026 Co",
            "38": "Adjusted (Real\u2011World) Estimate (via Llama 3.1 comparison): ~2.79 GPU hours\nDeepSeek\u2011V3 Reported Breakdown:\nPre\u2011training: 2,664K GPU hours\nContext Extension: 119K GPU hours\nPost\u2011training: 5K GPU hours\nTotal: ~2.788 M GPU hours\n### Cost (at $2 per GPU hour): ~$5.576 million",
            "39": "# Design Notes\n\n## Design and implementation\n\nThe 3FS system has four components: cluster manager, metadata service, storage service and client. All components are connected in an RDMA network (InfiniBand or RoCE).\n\nMetadata and storage services send heartbeats to cluster manager. Cluster manager ha",
            "40": "File metadata operations (e.g. open or create files\/directories) are sent to metadata services, which implement the file system semantics. Metadata services are stateless, since file metadata are stored in a transactional key-value store (e.g. FoundationDB). Clients can connect to any metadata servi",
            "41": "## File system interfaces\n\nObject store is becoming a popular option for data analytics and machine learning. However, file system semantics and a unified namespace where files are organized in directories provide greater flexibility for applications.\n\n-   *Atomic directory manipulation* An object s",
            "42": "-   *Symbolic and hard links* Our applications utilize symbolic and hard links to create lightweight snapshots of dynamically updated datasets, where new data is appended as individual files.\n\n-   *Familiar interface* The file interface is well known and used everywhere. There is no need to learn a ",
            "43": "-   *Memory copy overhead* The user-space file system daemon cannot access application memory. Data transfer between kernel and user spaces consumes memory bandwidth and increases end-to-end latency.\n\n-   *Primitive multi-threading support* When an application initiates I\/O requests, FUSE places the",
            "44": "Most applications, e.g. data analytics, perform large block writes on 3FS or they can buffer data in memory and flush it to 3FS when write buffer is full. However, FUSE on Linux 5.x does not support concurrent writes to the same file[^1]. Applications overcome this limitation by writing to multiple ",
            "45": "### Asynchronous zero-copy API\n\nImplementing the file system client as a VFS kernel module avoids performance issues mentioned above. But kernel module development is significantly more challenging than user-space system programming. Bugs are difficult to diagnose and can lead to catastrophic failur",
            "46": "For these reasons, we have chosen to implement a native client within the FUSE daemon. This client offers an interface that supports asynchronous zero-copy I\/O operations. File meta operations are still handled by FUSE daemon (e.g. open\/close\/stat files). Applications call `open()` to obtain a file ",
            "47": "-   *Ior* A small shared ring buffer for communication between user process and native client. The usage of Ior is similar to Linux `io_uring`, where the user process enqueues read\/write requests, and the native client dequeues these requests for completion. The requests are executed in batches, wit",
            "48": "## File metadata store\n\n### Location of file chunks\n\n3FS divides file data into equally sized chunks and stripes them across multiple replication chains (replication chains and chain tables are defined in Section [Data placement](#data-placement)). Users can specify the chain table, chunk size, and ",
            "49": "When an application opens a file, the client contacts the meta service to obtain the file\u2019s data layout information. Then the client can independently compute chunk IDs and chains for data operations, minimizing the involvement of the meta service in the critical path.\n\n### File metadata on transact"
        }
    }
    # Combine all question and answer texts into a single list, cleaned
    texts = [clean_text(data["question"][str(i)]) for i in range(50)] + \
            [clean_text(data["answer"][str(i)]) for i in range(50)]
    return Dataset.from_dict({"text": texts})

# 3. Preprocessing Function
def preprocess_function(examples, tokenizer, max_length=128):
    tokenized = tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=max_length,
    )
    tokenized["labels"] = tokenized["input_ids"].copy()  # For causal LM
    return tokenized

# Main Script
def main():
    # Model and Tokenizer Setup
    model_name = "Qwen/Qwen2.5-3B-Instruct"
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=model_name,
        max_seq_length=2048,
        dtype=torch.float16,
        load_in_4bit=True,
    )

    # Add LoRA Adapters
    model = FastLanguageModel.get_peft_model(
        model,
        r=16,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
        lora_alpha=32,
        lora_dropout=0,
    )

    # Load and Clean Dataset
    dataset = get_dataset()
    print(f"Loaded dataset with {len(dataset)} examples.")

    # Split into Train and Test (80/20)
    train_test_split = dataset.train_test_split(test_size=0.2, seed=42)
    train_dataset = train_test_split["train"]
    test_dataset = train_test_split["test"]
    print(f"Train size: {len(train_dataset)}, Test size: {len(test_dataset)}")

    # Preprocess Datasets
    train_dataset = train_dataset.map(
        lambda examples: preprocess_function(examples, tokenizer),
        batched=True,
    )
    test_dataset = test_dataset.map(
        lambda examples: preprocess_function(examples, tokenizer),
        batched=True,
    )
    print("Dataset preprocessed. Sample train example:", train_dataset[0])

    # Training Arguments
    training_args = TrainingArguments(
        output_dir="./qwen_finetuned",
        num_train_epochs=3,
        per_device_train_batch_size=2,
        per_device_eval_batch_size=2,
        gradient_accumulation_steps=4,
        logging_dir="./logs",
        report_to="none",
        fp16=True,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
    )

    # Initialize Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        processing_class=tokenizer,
    )

    # Train the Model
    print("Starting training...")
    trainer.train()

    # Evaluate the Model
    eval_results = trainer.evaluate()
    print("Evaluation results:", eval_results)

    # Test Generation
    model.eval()
    test_input = tokenizer("Tell me about DeepSeek-V3", return_tensors="pt").to("cuda")
    with torch.no_grad():
        output = model.generate(**test_input, max_new_tokens=50)
    print("Generated output:", tokenizer.decode(output[0], skip_special_tokens=True))

    # Save Quantized Model
    output_dir = "./qwen_finetuned_gguf"
    print("Saving quantized model to", output_dir)
    model.save_pretrained_gguf(output_dir, tokenizer, quantization_method="q4_k_m")

    # Verify GGUF File
    gguf_file = os.path.join(output_dir, "unsloth.Q4_K_M.gguf")
    if os.path.exists(gguf_file):
        file_size = os.path.getsize(gguf_file) / (1024 * 1024)  # Size in MB
        print(f"GGUF file saved successfully: {gguf_file} ({file_size:.2f} MB)")
    else:
        print("Error: GGUF file not found!")

    # Documentation
    with open("training_report.txt", "w", encoding="utf-8") as f:
        f.write("Fine-Tuning Report\n")
        f.write(f"Model: {model_name}\n")
        f.write(f"Dataset: 100 examples (50 questions + 50 answers) from DeepSeek-V3 documentation\n")
        f.write(f"Train/Test Split: {len(train_dataset)}/{len(test_dataset)}\n")
        f.write(f"LoRA: r=16, trainable params = 0.41%\n")
        f.write(f"Training: {training_args.num_train_epochs} epochs, batch size {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\n")
        f.write(f"Evaluation Results: {eval_results}\n")
        f.write(f"Quantized Model: {gguf_file}, {file_size:.2f} MB\n")
    print("Training report saved to training_report.txt")

if __name__ == "__main__":
    main()